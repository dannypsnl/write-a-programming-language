var documenterSearchIndex = {"docs":
[{"location":"chapter_4/#Chapter-4:-Polymorphism-and-Advanced-Type-Inference","page":"Chapter 4: Polymorphism & Advanced Type Inference","title":"Chapter 4: Polymorphism & Advanced Type Inference","text":"","category":"section"},{"location":"chapter_4/","page":"Chapter 4: Polymorphism & Advanced Type Inference","title":"Chapter 4: Polymorphism & Advanced Type Inference","text":"Polymorphism is all about how to handle more generally input via some constraints. Here we are going to talk about","category":"page"},{"location":"chapter_4/","page":"Chapter 4: Polymorphism & Advanced Type Inference","title":"Chapter 4: Polymorphism & Advanced Type Inference","text":"Chapter 4-1: Ad-hoc polymorphism\nChapter 4-2: Parametric polymorphism\nChapter 4-3: Subtyping\nChapter 4-4: row polymorphism","category":"page"},{"location":"chapter_8/#Chapter-8:-Curry-Howard-Correspondence","page":"Chapter 8: Curry-Howard Correspondence","title":"Chapter 8: Curry-Howard Correspondence","text":"","category":"section"},{"location":"chapter_8/","page":"Chapter 8: Curry-Howard Correspondence","title":"Chapter 8: Curry-Howard Correspondence","text":"Curry-Howard correspondence is all about correspondencing between types and logic.","category":"page"},{"location":"chapter_8/","page":"Chapter 8: Curry-Howard Correspondence","title":"Chapter 8: Curry-Howard Correspondence","text":"Sigma\n: exists\nPi\n: forall\nrightarrow\n: implies","category":"page"},{"location":"chapter_8/","page":"Chapter 8: Curry-Howard Correspondence","title":"Chapter 8: Curry-Howard Correspondence","text":"By this correspondence, we can write constructive proof by the program.","category":"page"},{"location":"chapter_3/#Chapter-3:-Lambda-Calculus","page":"Chapter 3: Lambda Calculus","title":"Chapter 3: Lambda Calculus","text":"","category":"section"},{"location":"chapter_3/","page":"Chapter 3: Lambda Calculus","title":"Chapter 3: Lambda Calculus","text":"lambda calculus created by three rules","category":"page"},{"location":"chapter_3/","page":"Chapter 3: Lambda Calculus","title":"Chapter 3: Lambda Calculus","text":"variable x\nabstraction lambda xM\napplication M  N","category":"page"},{"location":"chapter_3/","page":"Chapter 3: Lambda Calculus","title":"Chapter 3: Lambda Calculus","text":"With two types: Base type and Arrow type, lambda calculus became Simply typed lambda calculus(STLC), the most important property of STLC was strong normalization, which means it produces type for all terms.","category":"page"},{"location":"chapter_3/","page":"Chapter 3: Lambda Calculus","title":"Chapter 3: Lambda Calculus","text":"STLC puts type rules in to system, where points out an abstraction lambda x  AM has type A to B if M has type B, and application M  N get type B if M has type A to B and N has type A.","category":"page"},{"location":"ch4/row-poly/#row-polymorphism","page":"row polymorphism","title":"row polymorphism","text":"","category":"section"},{"location":"ch4/row-poly/","page":"row polymorphism","title":"row polymorphism","text":"Row polymorphism helps us get a structure only when it with expected properties. For example, we can have a function type: {a : A, rest : ...} -> {a : A}, where {a = A()}, {a = A(), b = B(), c = C()}, and {a = A(), b = B()} are all valid argument, but {a = B()}, {b = B()}, and {b = B(), c = C()} are not.","category":"page"},{"location":"ch4/ad-hoc-poly/#Ad-hoc-polymorphism","page":"Ad-hoc polymorphism","title":"Ad-hoc polymorphism","text":"","category":"section"},{"location":"ch4/ad-hoc-poly/","page":"Ad-hoc polymorphism","title":"Ad-hoc polymorphism","text":"Ad-hoc can be simply understandsing as overloading (though that's to simple but enough for now), in Haskell you have class (not class in C++/Java), in Java you have overload function, there has no different down to the core. This feature allows a function definition works with different type. Two concrete approaches(Haskell and Java style) looks different at first:","category":"page"},{"location":"ch4/ad-hoc-poly/","page":"Ad-hoc polymorphism","title":"Ad-hoc polymorphism","text":"Haskell version\nclass Countable a where\n  count :: a -> Int\n\ninstance Countable [a] where\n  count lst = length lst\n\ninstance Countable Human where\n  count Human { age=age } = age\nJava version\nint count(List<T> lst) { return lst.size(); }\nint count(Human h) { return h.getAge(); }","category":"page"},{"location":"ch4/ad-hoc-poly/","page":"Ad-hoc polymorphism","title":"Ad-hoc polymorphism","text":"Both implemented by the same way:","category":"page"},{"location":"ch4/ad-hoc-poly/","page":"Ad-hoc polymorphism","title":"Ad-hoc polymorphism","text":"using encoding duplicate functions' name(ignore parametric polymorphism for now) and record these instances, for example:\nHaskell get: Countable.[a].count :: [a] -> Int, Countable.Human.count :: Human -> Int\nJava get: count_List<T>, count_Human\nsearch instances for function call, for example:\ncount [1, 2, 3] get Countable.[a].count :: [a] -> Int\ncount 1 get nothing and hence get an error","category":"page"},{"location":"ch4/ad-hoc-poly/","page":"Ad-hoc polymorphism","title":"Ad-hoc polymorphism","text":"How about dynamic typing? Dynamic typed language still implements ad-hoc since they simply accept all types, which means the previous two-steps method didn't need by them, but the cost is them would crash in runtime if no instance.","category":"page"},{"location":"chapter_7/#Chapter-7:-Dependent-Type","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"","category":"section"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"Lambda Cube demonstrate an interesting picture about type theory world, but it just a start, Calculus of Construction needs to consider reality to work. We can imagine a function like the following.","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"(define (A-or-B [b : Bool] [A B : Type]) : Type\n  (match b\n    [true => A]\n    [false => B]))","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"Obviously,","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"(A-or-B true Nat Bool) produces Nat\n(A-or-B false Nat Bool) produces Bool","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"Thus, we can have definition","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"(define a : (A-or-B true Nat Bool)\n  1)","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"which very make sense.","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"Until we found something the following.","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"(define (endless [n : Nat]) : Type\n  (match n\n    [zero => Bool]\n    [(suc ,n) => (endless (suc n))]))","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"(endless zero) produces Bool\nHowever, (endless (suc n)) for any n : Nat produces nothing!","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"The computation even won't get stop! Then our type checking just a joke since halting problem is undecidable.","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"Thus, we need termination check.","category":"page"},{"location":"chapter_7/#Termination-Check","page":"Chapter 7: Dependent Type","title":"Termination Check","text":"","category":"section"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"The simplest solution was ensuring program could be converted to eliminator, the only weak point was this approach couldn't solve more complicated case but only primitive pattern(only expand one level for all constructors of an inductive type). For example","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"(define (+ [n m : Nat]) : Nat\n  (match n\n    [zero => m]\n    [(suc ,n) => (suc (+ n m))]))","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"Another approach is ensuring no greater construction in expression of pattern by syntax checking(I remember this one is the first showed approach). For example, (suc (suc n)) would get rejected with pattern (suc ,n).","category":"page"},{"location":"chapter_7/","page":"Chapter 7: Dependent Type","title":"Chapter 7: Dependent Type","text":"A more recently solution was Mini-Agda's sized type, this is associating a constructor with a number and some arithmetic rules to judge which is greater construction. For example, if (suc ,n) has size k(a number), then n has size k-1, therefore, (suc (suc n)) has k+1 is greater than origin pattern. This approach can even handle cross function tracing.","category":"page"},{"location":"chapter_10/#Chapter-10:-Refinement-Type","page":"Chapter 10: Refinement Type","title":"Chapter 10: Refinement Type","text":"","category":"section"},{"location":"chapter_9/#Chapter-9:-Substructural-Type","page":"Chapter 9: Substructural Type","title":"Chapter 9: Substructural Type","text":"","category":"section"},{"location":"appendix_parser/#Parser","page":"Parser","title":"Parser","text":"","category":"section"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"I have to say I have no idea why most compiler books spent time on Parser, of course this is really complex topic, but practical language usually didn't need the complex techiology. But at here I still would list the common tools would be there for developing a Parser.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Before that, we must know why we need the Parser, the Parser was a tool to translate the language in our mind into another language to run on certain environment. The most common example was compiler compile the language to assembly, but why assembly language? Because the environment in case was OS provide the tool called assembler which can translate assembly to machine code which can run on the real machine(or more specific, run by CPU). Or like TypeScript, Elm and GHCJs compile the origin language to another high-level language JavaScript.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"So the point was we would like to run a program, and we would find out how to run it on our target platform. When you learn these stuffs, I believe there are many resources keep mention AST. But what's AST? AST was abstract syntax tree, or to be honestly, our real language. Sure, the syntax was not language, or at least just the outlooking part. The all important things are our mind concept, and the language just a way to express our mind with some trade-off. What trade-off? For example a language can write:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"print hello, world","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Of course was possible, however, would be hard to read in more complex place. So rather than make Compiler handles it, we choose to let people(programmer) handles string. Now the code became:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"print \"hello, world\"","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"It looks more clear. But the example can be more complex, remember currently we are say print takes \"hello, world\" to do something, now we extend the example:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"print \"hello, \" user_input","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"user_input was another function which get input from user, no matter how did it work, now we have trouble: We should print hello, <function> or hello, Danny(if user type in Danny)?","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"In fact, compiler can not do the decision for you, whatever which behavior it picked would make functionality missing. Sometimes we are really want to print out the function value. So we introduce parenthesis in case:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"print(\"hello, \", user_input())\n# or\nprint \"hello, \" (user_input)","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"The first one was picked by C family, and second one was using by ML family, both has cons and pros. We would keep mention these issues.","category":"page"},{"location":"appendix_parser/#Simple-parser-and-why-we-have-next-section","page":"Parser","title":"Simple parser and why we have next section","text":"","category":"section"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"In this section we would use Perl6 to build a parser. Parser can be generated? Sure, but I do not recommend it in production. But for simple stuff it was fine.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Here we were going to talk about natural number arithmetic syntax which supprts plus: +, times(multiple): *, minus: - and divide: /.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"grammar Calculator {\n    token TOP { <calc-op> }\n\n    proto rule calc-op          {*}\n          rule calc-op:sym<mult> { <num> '*' <num> }\n          rule calc-op:sym<div> { <num> '/' <num> }\n          rule calc-op:sym<add> { <num> '+' <num> }\n          rule calc-op:sym<sub> { <num> '-' <num> }\n    # just like regex, \\d+ is at least one digit\n    token num { \\d+ }\n}","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"This is a very short syntax, even C lanugage syntax has 954 lines: https://github.com/antlr/grammars-v4/blob/master/c/C.g4 , cpp even has 1940 lines: https://github.com/antlr/grammars-v4/blob/master/cpp/CPP14.g4 in Antlr4(another parser generator).","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Forget about that, at here code generator was really useful, we can quickly generate the Parser for our purpose. Then we can create an interpreter based on it:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"class Calculations {\n    method TOP              ($/) { make $<calc-op>.made; }\n    # if you are not familiar with Perl just like me, notice that `calc-op` mapping to each grammar\n    method calc-op:sym<mult> ($/) { make [*] $<num> }\n    method calc-op:sym<div> ($/) { make [/] $<num> }\n    method calc-op:sym<add> ($/) { make [+] $<num>; }\n    method calc-op:sym<sub> ($/) { make [-] $<num>; }\n}\n\nsay '2+2 = ' ~ Calculator.parse('2+2', actions => Calculations).made;\nsay '2*3 = ' ~ Calculator.parse('2*3', actions => Calculations).made;","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"This one basically not good enough, it can't handle parentheses, can't handle 2 * 2 + 3. But anyway shows how interpreter work.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Now consider a manual parser, how would it looks like? It actually easy to build up. Consider the following Java program:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Scanner s = new Scanner(input);\nStringBuilder number = new StringBuilder(\"\");\nchar c = s.next().charAt(0);\nwhile (Character.isDigit(c)) {\n    number.append(c);\n    c = s.next().charAt(0);\n}\n// now c must not a digit\nwhile (Character.isSpace(c)) {\n    c = s.next().charAt(0); // skip whitespace\n}\nif (c == '+') {\n    c = s.next().charAt(0);\n} else {\n    throw new SyntaxException(\"allow + operator only\");\n}\nwhile (Character.isSpace(c)) {\n    c = s.next().charAt(0); // skip whitespace\n}\nStringBuilder number2 = new StringBuilder(\"\");\nchar c = s.next().charAt(0);\nwhile (Character.isDigit(c)) {\n    number2.append(c);\n    c = s.next().charAt(0);\n}\nreturn new BinaryExpression(number, number2, Operator(\"+\"));","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"This is of course too exaggerated, but can show why handling input stream is not a good idea. That's why we introduce Lexer layer.","category":"page"},{"location":"appendix_parser/#Lexer","page":"Parser","title":"Lexer","text":"","category":"section"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Lexer is an optional, the correct way to describe it was a helper component, we would need it when the token was trying to reduce the concept we have to consider. If we don't use lexer, when we parsing","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"class Foo {}","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"we could write down:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"identifier = take_char_until_one_of([' ', '\\n'])\nif identifier == \"class\":\n    name_of_class = take_char_until_one_of([' ', '\\n'])\n    require('{')\n    require('}')\nelse:\n    # parse different rule","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Which was very low-level program, we have to handle each space and newline and remember when we don't need them. For many language we can extract out lexer/tokenizer to do these. The idea was we don't have to directly work with string, but with token, a token could contain location, content, type these information to help parser keep doing the parsing. A lexer can directly skip whitespace and newline, update location information and normalize the content of token(for example we can parse int or parse float before the token sent to parser).","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Here, I show a naive Lexer written in Rust:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"// first need to define Location\n#[derive(Clone, Debug)]\nstruct Location {\n    file_name: String,\n    // line, column is the pair of location, use start point of token\n    line: u32,\n    column: u32,\n    // start, end is the start offset to end offset for a token, for example: `let` at 0 has start: 0 and end: 3\n    start: u32,\n    end: u32,\n}\n\nimpl Location {\n    pub fn new<T: ToString>(\n        file_name: T,\n        line: u32,\n        column: u32,\n        start: u32,\n        end: u32,\n    ) -> Location {\n        Location {\n            file_name: file_name.to_string(),\n            line,\n            column,\n            start,\n            end,\n        }\n    }\n}\n\n// then need some Token Type\n#[derive(Clone, Debug, PartialEq)]\nenum TkType {\n    EOF,\n    Identifier,\n    KeywordLet,\n    Integer,\n}\n\n// A Token has location, type and value\nstruct Token(Location, TkType, String);\n\n// A lexer moving between states, by each state has a behavior produce new state, and at the end must reach EOF to complete tokenizing, ok, so we can have such definition:\nenum State {\n    Fn(fn(&mut Lexer) -> State),\n    EOF,\n}\n\n// Now define Lexer\nstruct Lexer {\n    file_name: String,\n    code: Vec<char>,\n    tokens: Vec<Token>,\n    state_fn: State,\n    start: usize,\n    offset: usize,\n    // (line, pos) represent the position for user\n    pos: u32,\n    line: u32,\n}\n\n// several helpers\nimpl Lexer {\n    fn new<T: Into<String>>(file_name: T, code: T) -> Lexer {\n        Lexer {\n            file_name: file_name.into(),\n            code: code.into().chars().collect(),\n            tokens: vec![],\n            state_fn: State::Fn(whitespace),\n            start: 0,\n            offset: 0,\n            pos: 0,\n            line: 1, // line start from 1\n        }\n    }\n\n    fn ignore(&mut self) {\n        self.pos += (self.offset - self.start) as u32;\n        self.start = self.offset;\n    }\n    fn peek(&self) -> Option<char> {\n        match self.code.get(self.offset) {\n            Some(c) => Some(*c),\n            None => None,\n        }\n    }\n    fn next(&mut self) -> Option<char> {\n        self.offset += 1;\n        self.peek()\n    }\n    fn new_token(&mut self, token_type: TkType, value: String) -> Token {\n        Token(\n            Location::new(\n                self.file_name.clone(),\n                self.line,\n                self.pos,\n                self.start as u32,\n                self.offset as u32,\n            ),\n            token_type,\n            value,\n        )\n    }\n    fn emit(&mut self, token_type: TkType) {\n        let s: String = self.code[self.start..self.offset].into_iter().collect();\n        let tok = match s.as_str() {\n            \"let\" => self.new_token(TkType::KeywordLet, s),\n            _ => self.new_token(token_type.clone(), s),\n        };\n        self.tokens.push(tok);\n        self.ignore();\n    }\n}\n\nfn whitespace(lexer: &mut Lexer) -> State {\n    while let Some(c) = lexer.peek() {\n        if c == ' ' || c == '\\r' || c == '\\n' {\n            if c == '\\n' {\n                lexer.next();\n                lexer.start = lexer.offset;\n                lexer.pos = 0;\n                lexer.line += 1;\n            } else {\n                lexer.next();\n            }\n        } else {\n            break;\n        }\n    }\n    lexer.ignore();\n\n    match lexer.peek() {\n        Some(_c @ '0'..='9') => State::Fn(number),\n        Some(c) => {\n            if in_identifier_set(c) {\n                State::Fn(ident)\n            } else {\n                unimplemented!(\"char: `{}`\", c);\n            }\n        }\n        None => State::EOF,\n    }\n}\n\nfn in_identifier_set(c: char) -> bool {\n    c.is_alphanumeric() || c == '_'\n}\nfn ident(lexer: &mut Lexer) -> State {\n    while let Some(c) = lexer.next() {\n        if !in_identifier_set(c) {\n            break;\n        }\n    }\n    lexer.emit(TkType::Identifier);\n    State::Fn(whitespace)\n}\nfn number(lexer: &mut Lexer) -> State {\n    while let Some(c) = lexer.next() {\n        if !c.is_digit(10) {\n            break;\n        }\n    }\n    lexer.emit(TkType::Integer);\n    State::Fn(whitespace)\n}\n\npub fn lex<T: Into<String>>(file_name: T, source: T) -> Vec<Token> {\n    let mut lexer = Lexer::new(file_name, source);\n    // tokenizing is just moving between state when possible\n    while let State::Fn(f) = lexer.state_fn {\n        lexer.state_fn = f(&mut lexer);\n    }\n    // emit final EOF to help Parser report EOF problem(optional, also can use no more token as EOF)\n    lexer.emit(TkType::EOF);\n    lexer.tokens\n}","category":"page"},{"location":"appendix_parser/#Manual-parser","page":"Parser","title":"Manual parser","text":"","category":"section"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"A manual parser is powerful, but on the other hand it takes a lot of effort. Before you jump into writing a manual parser and never go back again, ensure that parser generator cannot handle your case.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Write a manual parser didn't need many parsing background knowledge, surprising, but heavy repetitive work. Because simply porting LL syntax can handle about 90% job. For example, an assignment syntax <type> <name> = <expr>; can use the following parser:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"typ = parse_type()\nname = parse_identifier()\nexpect_symbol('=')\nexpr = parse_expr()\nexpect_symbol(';')\nreturn Assign(typ=typ, name=name, expr=expr)","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"However, there has an annoying case: expression parsing. How this became the big problem for newbies? If we follow LL strict conversion from syntax as below:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"expr ::=\n  expr \"*\" expr\n  | expr \"+\" expr\n  | // ignore others","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"the conversion is:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"def parse_expr():\n    left_expr = parse_expr()\n    op = expect_oneof('*', '+')\n    right_expr = parse_expr()\n    return BinaryExpression(left_expr, op, right_expr)","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"This is left recursive, your parser would keep calling parse_expr and never end(or stack overflow, depend on which language you're using).","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Clever as you, might thought out how to fix this quickly:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"expr ::=\n  integer \"*\" expr\n  | integer \"+\" expr\n  | integer","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"here is conversion:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"def parse_expr():\n    left_expr = parse_integer()\n    try:\n        op = expect_oneof('*', '+')\n        try:\n            right_expr = parse_expr()\n            return BinaryExpression(left_expr, op, right_expr)\n        except:\n            # keep throw up the parse error\n            raise\n    except:\n        # to simplify example, assuming `expect_oneof` is under the control and won't throw unexpected exception\n        # no right hand side expression\n        return left_expr","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"It would work, until you have to handle operator precedence. For example, * would usually be applied before +. As clever as you, get the solution quickly again:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"expr ::=\n  term \"*\" expr\n  | term\nterm ::=\n  integer \"+\" expr\n  | integer","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"However, the implementation work became quick heavy now.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"To solve all problem in once, we need a better way to handle this, and that is operator precedence parser(provided by Wiki):","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"parse_expression()\n    return parse_expression_1(parse_primary(), 0)\nparse_expression_1(lhs, min_precedence)\n    lookahead := peek next token\n    while lookahead is a binary operator whose precedence is >= min_precedence\n        op := lookahead\n        advance to next token\n        rhs := parse_primary ()\n        lookahead := peek next token\n        while lookahead is a binary operator whose precedence is greater\n                 than op's, or a right-associative operator\n                 whose precedence is equal to op's\n            rhs := parse_expression_1 (rhs, lookahead's precedence)\n            lookahead := peek next token\n        lhs := the result of applying op with operands lhs and rhs\n    return lhs","category":"page"},{"location":"appendix_parser/#Combinator","page":"Parser","title":"Combinator","text":"","category":"section"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Finally, my favourite technology is combinator, here is the reason:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"#lang racket\n\n(require data/monad data/applicative) ;;; raco pkg install functional-lib\n(require megaparsack megaparsack/text) ;;; raco pkg install megaparsack\n\n(define lexeme/p\n  ;;; lexeme would take at least one space or do nothing\n  (do (or/p (many+/p space/p) void/p)\n    (pure (λ () 'lexeme))))\n(define (keyword/p keyword)\n  (do (string/p keyword)\n    (lexeme/p)\n    (pure keyword)))\n(define identifier/p\n  (do [id <- (many+/p letter/p)]\n    (lexeme/p)\n    (pure (list->string id))))\n(define (type/p ctx)\n  (do [check-struct <- (or/p (keyword/p \"struct\") void/p)]\n    [typ <- identifier/p]\n    (pure ((λ ()\n             (context/lookup-type-id ctx typ (eqv? check-struct \"struct\")))))))\n\n(define (struct-field/p ctx)\n  (do [field <- (list/p (type/p ctx) identifier/p)]\n    (char/p #\\;)\n    (lexeme/p)\n    (pure field)))\n(define (struct-def/p ctx)\n  (do (keyword/p \"struct\")\n    [name <- identifier/p]\n    (char/p #\\{)\n    (lexeme/p)\n    [fields <- (many/p (struct-field/p ctx))]\n    (lexeme/p)\n    (char/p #\\})\n    (pure ((λ ()\n             (context/new-type ctx name (CStruct name fields))\n             (CStructDef name fields))))))","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"This just show how to parse a C structure definition using Racket combinator lib: megaparsack. Basically just as BNF definition, but with action easier and still using original language is the point.","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Quickly the problem would be how to make operator precedence parsing, because combinator doesn't good at loop directly, however, combinator good at recursive:","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"#lang racket\n\n(require data/monad data/applicative)\n(require megaparsack megaparsack/text)\n\n(define lexeme/p\n  ;;; lexeme would take at least one space or do nothing\n  (do (or/p (many+/p space/p) void/p)\n    (pure (λ () 'lexeme))))\n\n(define (op/p op-list)\n  (or/p (one-of/p op-list)\n        void/p))\n(define factor/p\n  (do [expr <- integer/p]\n    (lexeme/p)\n    (pure expr)))\n(define (binary/p high-level/p op-list)\n  (do [e <- high-level/p]\n    ; `es` parse operator then high-level unit, for example, `* 1`.\n    ; therefore, this parser would stop when the operator is not expected(aka. operator is in op-list)\n    ; rely on this fact we can leave this loop\n    [es <- (many/p (do [op <- (op/p op-list)]\n                     (lexeme/p)\n                     [e <- high-level/p]\n                     (pure (list op e))))]\n    (pure (foldl\n           (λ (op+rhs lhs)\n             (match op+rhs\n               [(list op rhs)\n                (list op lhs rhs)]))\n           e es))))\n(define (table/p base/p list-of-op-list)\n  (if (empty? list-of-op-list)\n      base/p\n      (table/p (binary/p base/p (car list-of-op-list))\n               (cdr list-of-op-list))))\n(define expr/p\n  (table/p factor/p\n           '((#\\* #\\/)\n             (#\\+ #\\-))))","category":"page"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"The code shows how to define table parser, and it even more simple to extend, all we need to do is just add new operator list into the table.","category":"page"},{"location":"appendix_parser/#Conclusion","page":"Parser","title":"Conclusion","text":"","category":"section"},{"location":"appendix_parser/","page":"Parser","title":"Parser","text":"Now we already shows all technologies for parsing(at least, what I know), now you can make some simple language by your own, but let me warn you: make a language doesn't easy, make it be runnable is just the first step, you need to make editor plugin(and we can't naively rely on parser I tell you here, since they cannot partially handle syntax), powerful debugger, profiler, and many other things to help people work on this language productively.","category":"page"},{"location":"chapter_6/#Chapter-6:-Lambda-Cube","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"","category":"section"},{"location":"chapter_6/","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"Lambda Cube, as its name, was a cube. The vertex of cube was type system, each edge shows a change to type system as a direction, and take a type system as source to produce another type system as target.","category":"page"},{"location":"chapter_6/","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"Its starts from STLC, we have three directions can pick.","category":"page"},{"location":"chapter_6/","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"polymorphism: lambda 2\nAlso known as system F, this allows terms to depend on types, by the following rule.\n$\n\\frac {\\Gamma \\vdash t : B} {\\Gamma \\vdash \\Lambda a.t : \\Pi a. B} $\nFor example, the following program\n(define (id x) x)\nwould have a type: ?a -> ?a. We can postpone evaluation to get what's ?a by unification algorithm. The output of (id k) depends on the type of k.\ndependent type: \\(\\lambda Pi\\)(lambda Pi or lambda P)\nThis system allows type depends on term, by the following rule.\n$\n\\frac {\\Gamma , x : A \\vdash B : *} {\\Gamma \\vdash (\\Pi x : A . B) : *} $\nA : * says A is a type, this rule says: when\nIn context, x is a A\nB is a type make sense\nthen Pi x  A  B is a type in such context.\ntype operator: lambda omega(lambda omega)\nThis system provides type depends on type, for example: (List A), (Tree A).","category":"page"},{"location":"chapter_6/","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"Combine them we can get","category":"page"},{"location":"chapter_6/","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"F omega\nlambda Pi 2\nlambda Piomega","category":"page"},{"location":"chapter_6/","page":"Chapter 6: Lambda Cube","title":"Chapter 6: Lambda Cube","text":"and all of them lambda C, the calculus of construction system, in following chapters dependent type usually just refer to this system rather than lambda Pi.","category":"page"},{"location":"ch4/parametric-poly/#Parametric-polymorphism","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"","category":"section"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"Parametric polymorphism also known as generic, it provides ability to abstract a type, in STLC we have abstraction which looks like lambda x  T M. If x is a type? We usually use * for type of type. Now we get lambda x  * M, an abstraction of a type. For example, List<T> is a function List : (T : *) -> *.","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"From implementation perspective, we have two solutions:","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"dynamic\ngenerate","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"Dynamic is the simple one for code generation, from C perspective it might just a void *, thus","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"struct Node<T> {\n    Node<T> *prev;\n    T value;\n}","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"would be:","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"struct Node {\n    Node *prev;\n    void *value;\n}","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"Generate is hard one, but can get better performance, still the same Node<T>, we generate Node<i64> to:","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"struct Node_i64 {\n    Node_i64 *prev;\n    i64 value;\n}","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"Node<f64> to:","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"struct Node_f64 {\n    Node_f64 *prev;\n    f64 value;\n}","category":"page"},{"location":"ch4/parametric-poly/","page":"Parametric polymorphism","title":"Parametric polymorphism","text":"However, problem of Generate method is obviously, it produces larger binary, and the performance issue of Dynamic method can be solved by JIT.","category":"page"},{"location":"chapter_2/#Chapter-2:-Type-Inference","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"","category":"section"},{"location":"chapter_2/","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"With type, we need to define all our variable like the following code:","category":"page"},{"location":"chapter_2/","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"i :: Integer\ni = 1\n\nb :: Boolean\nb = True\n\nc :: Char\nc = 'c'","category":"page"},{"location":"chapter_2/","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"This is annoying since we can see the type from primitive terms! Which means it's ok to rewrite above as the following, if your language supports type inference!","category":"page"},{"location":"chapter_2/","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"i = 1\nb = True\nc = 'c'","category":"page"},{"location":"chapter_2/","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"A simple inference function for your language might like","category":"page"},{"location":"chapter_2/","page":"Chapter 2: Type Inference","title":"Chapter 2: Type Inference","text":"infer :: Term -> Type\ninfer (TmInt _) = TyInt\ninfer (TmBool _) = TyBool\ninfer (TmChar _) = TyChar","category":"page"},{"location":"chapter_1/#Chapter-1:-Type-Checking","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"","category":"section"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"Without type checking, program still works. Then why type checking? The process spent times, right? Yes, type checking uses more resource if we always make correct software. Unfortunately, we don't. For example, we might exceptionally write:","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"\"hello, \" ++ 1","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"Who is 1? That's a bit ridiculous unless we start naming people in number. How about hello name = \"hello, \" ++ name? This program can happen, or we write it again, but with check (in untyped scheme):","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"(define (hello name)\n  (when (string? name)\n    (string-append \"hello, \" name)))","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"This time we always have a string, ignores others, but maybe we would like to know this function failed?","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"(define (hello name)\n  (if (string? name)\n    (string-append \"hello, \" name)\n    (error 'hello \"name should be a string!\")))","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"Or we check it when compiling","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"hello :: String -> String\nhello name = \"hello, \" ++ name","category":"page"},{"location":"chapter_1/","page":"Chapter 1: Type Checking","title":"Chapter 1: Type Checking","text":"This is correct answer, if we only care about the runtime speed. With more type(information), better output from compiler was possible. Unfortunately, from another perspective, compile time can be hurt. Runtime performance is not the only consideration in the real world, what does it mean? In language like Coq, Agda, proof a list has expected length in compile time is possible, but they might take a crazy long time to compile a small program. Simpler languages like Rust, Haskell, still known as having slow compiler. More problems to solve, more time need. Designing a language, means make a balance between these choose, include but not just compile time problem.","category":"page"},{"location":"chapter_5/#Chapter-5:-Inference-code-with-polymorphic-type","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"","category":"section"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"You already learn type inference concept, but if you keep going, you will find the code in chapter 2 cannot handle many important case. With polymorphism, our type getting more horrible now. Can we avoid most type mark and still get benefit from the type system? The answer is no for polymorphic lambda calculus(also known as system F), for example, the following code cannot have fixed result","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"\\f -> f 1","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"Let's see how it goes to ","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"we know 1 has type Integer, but what's the result type of f? It can be f : Integer -> ?, but if we cannot ensure ? is what, we are not able to unify ? with any type. Thus, the inference on system F is not decidable. However, the following code is type checked","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"(\\f -> f 1) (\\x -> x)","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"We get f : a -> a from \\x -> x, then a = Integer when apply f with 1, hence (\\f -> f 1) (\\x -> x) : Integer. The most important observe here is: If we can detect the implementation of an appliable binding, then inference is decidable. Thus, Hindley-Milner type system introduces let binding into the system, which gives a such form.","category":"page"},{"location":"chapter_5/#Implementation","page":"Chapter 5: Inference code with polymorphic type","title":"Implementation","text":"","category":"section"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"The first part is about syntax recur-infer build type by traveling on the syntax tree, we give any unknown type a tag(free variable type) to keep who they're, such tag is encoding by a procedure for GEN and INST rule, and a parameter for Free. A binding must use INST rule to generate a clean new type instance, for example:","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"let id = \\x -> x\nin (id 1, id \"\")","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"id must need to be instantiated before using, else we would see error: cannot unify number and string since id unify its free variable with number first, then unify with string but sharing a same instance.","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"(define (recur-infer tm [env (make-immutable-hash)])\n  (match tm","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"Lambda rule is simple, a (-> (parameter-type* ...) return-type), but we didn't know the type of parameter, therefore, given ?0, ?1, ?2 and so on. Then create a new environment to infer its body.\n    [`(λ (,x* ...) ,t)\n     (let ([λ-env (foldl (λ (x e)\n                           (extend/env e x (make-parameter (gensym '?))))\n                         env x*)])\n       `(-> ,(map (λ (x) (recur-infer x λ-env)) x*)\n            ,(recur-infer t λ-env)))]\nlet rule, which seems like not need, is quite important. In Racket, a possible transformation is let to lambda, however, in HM system they are different as we say above. However, notice that we can make a trick: bind the inferred type to abstraction's parameter if it's an immediate application. Another way is introducing make polymorphism type can in the definition of parameter.\n    [`(let ([,x* ,xt*] ...) ,t)\n     (let ([let-env (foldl (λ (x t e)\n                             (extend/env e x\n                                         (λ () (recur-infer t e))))\n                           env x* xt*)])\n       (recur-infer t let-env))]\nlist are something like '(1 2 3), pair are (pair 1 2). In these cases, we return (<list or pair> ?) if no elements, we will not sure what's ?(use (make-parameter (gensym))) till we get some operations like: (append a-list 1) then infer ? via application rule. If there have elements, we infer via first element, and check rest elements!\n    [`(pair ,a ,b)\n     `(pair ,(recur-infer a env) ,(recur-infer b env))]\n    [`(quote ,p*)\n     `(list ,(if (empty? p*)\n                 (make-parameter (gensym '?))\n                 (let ([et (recur-infer (car p*) env)])\n                   (for-each (λ (et*) (unify et* et))\n                             (map (λ (x) (recur-infer x env)) (cdr p*)))\n                   et)))]\nApplication rule unify the f type with a new arrow(->) type which constructed by arguments' type, and a free type variable for return type. Then give final return type as its result.\n    [`(,f ,arg* ...)\n     (let ([free (make-parameter (gensym '?))])\n       (unify (recur-infer f env)\n              `(-> ,(map (λ (arg) (recur-infer arg env)) arg*) ,free))\n       free)]\nFinally, we get some simple type(monolithic)\n    [x (cond\n         [(string? x) 'string]\n         [(number? x) 'number]\n         [(char? x) 'char]\n         [(symbol? x)\n          (let ([t (lookup/type-of env x)])\n            (if (and (procedure? t) (not (parameter? t)))\n                (t)\n                t))]\n         [else (error (format \"unknown form: ~a\" x))])]))","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"Above program separate and explain how the key part working, in the last step, once we get all result, we remove all free variable as possible.","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"(define (elim-free ty)\n  (match ty\n    [`(,ty* ...)\n     (map elim-free ty*)]\n    [ty (if (parameter? ty)\n            (elim-free (ty))\n            ty)]))\n\n(define (infer tm) (elim-free (recur-infer tm)))","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"Then here was the key of all the stuff: occurs and unify, unification is all about binding variable with any order. Thus, (unify ?a int) and (unify int ?a) should produce same result and make ?a be int, and we also believe ?a cannot unify with string again since its int and int is not string. However, unifying ?a with ?b is an option, it has no different with (unify ?b int) and unify ?b ?a. The last thing we need to be careful was recursion, consider if we (unify ?a (list ?a)), our process would run into trouble: ?a is (list ?a), but what's (list ?a)? We expand ?a then get (list (list ?a)), but then what's ?a? Again and again... Thus, we must check ?a didn't occur in the type which it's going to bind.","category":"page"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"(define (occurs v t)\n  (match t\n    [`(,t* ...)\n     (ormap (λ (t) (occurs v t)) t*)]\n    (t (equal? v t))))\n\n(define (unify t1 t2)\n  (match* (t1 t2)\n    [(_ t2) #:when (and (parameter? t2)\n                        ;;; ensure t2 is still free\n                        (string-prefix? (symbol->string (t2)) \"?\"))\n            (when (or (eqv? t1 (t2)) (occurs (t2) t1))\n              (error (format \"~a occurs in ~a\" (t2) t1)))\n            (t2 t1)]\n    [(t1 _) #:when (parameter? t1)\n            (unify t2 t1)]\n    [(`(,a* ...) `(,b* ...))\n     (for-each unify a* b*)]\n    [(_ _)\n     (let ([a (elim-free t1)]\n           [b (elim-free t2)])\n       (unless (eqv? a b)\n         (error (format \"cannot unify type ~a and ~a\" a b))))]))","category":"page"},{"location":"chapter_5/#More","page":"Chapter 5: Inference code with polymorphic type","title":"More","text":"","category":"section"},{"location":"chapter_5/","page":"Chapter 5: Inference code with polymorphic type","title":"Chapter 5: Inference code with polymorphic type","text":"Though I said rebound is unacceptable, in fact we can make several variants of type systems on this, by introducing one of union type, higher rank type, row polymorphism, we can get lots of fun. Hindley-Milner type system is not a good system in practice what we already know, even Haskell best practice would tell you at least give the top level binding a type annotation. However, the core of Hindley-Milner type system, unification was really important in the more advanced type system like Dependent type since we need to deal with a lots of duplicate annotation in these variants.","category":"page"},{"location":"#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Originally, I follow most traditional course and trying to tell how to make a programming language from technology view, but I quickly found how wrong this way. Because we already have a lot of language, another bad language is not need. Existed tutorials already enough to help anyone make a compiler, but how to design a good language is a question, you would usually need to read tons of books in PLT(Programming Language Theory) this area, and be carefully to avoid be killed by math. I won't try to say what I made is the best, but giving a lot of independent features, and let readers feel the core concept behind them to help readers make a good design.","category":"page"},{"location":"ch4/subtyping/#Subtyping","page":"Subtyping","title":"Subtyping","text":"","category":"section"},{"location":"ch4/subtyping/","page":"Subtyping","title":"Subtyping","text":"Subtyping is a predicate between two types, describe a type can be treat as another type. Its formal syntax usually looked like this: A <: B(A is subtype of B), which means, a binding B can have value has type A. Subtyping was wildly used in many class-based languages, but it also brings new problem when working with Ad-hoc. For example: f : B -> B usually can apply with f (a : A) when has subtyping, however, if we also have overloading function f : A -> B? We have three choices here:","category":"page"},{"location":"ch4/subtyping/","page":"Subtyping","title":"Subtyping","text":"use A -> B\nuse B -> B\nreport conflict error","category":"page"},{"location":"ch4/subtyping/","page":"Subtyping","title":"Subtyping","text":"All of them are fine, it depends on which one you tend to have, this is just trade off.","category":"page"}]
}
